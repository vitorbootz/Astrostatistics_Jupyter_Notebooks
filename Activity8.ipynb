{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Afternoon Activity - Day 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This notebook was developed by [Zeljko Ivezic](http://faculty.washington.edu/ivezic/) for the 2021 data science class at the University of Sao Paulo and it is available from [github](https://github.com/ivezic/SaoPaulo2021/blob/main/notebooks/Activity8.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1 \n",
    "\n",
    "Use Kernel Density Estimation with a top-hat, Gaussian, and exponential kernel on the color-magnitude diagrams (CMDs) of the two data sets (`Field A` and `Field B`) from Homework 7. Make the analog of Figure 6.3 from the textbook (the code for that figure is included below), but with the CMDs in each panel (i.e., $g-r$ on the $x$, and $g$ on the $y$ axis). In astronomy, such figures are called Hess diagrams. \n",
    "\n",
    "Experiment with different kernel bandwidths, plotting one that visually seems \"best\" (i.e., a good balance of bias vs. variance) for each kernel.\n",
    "\n",
    "Don't forget to change the figure size so that individual panels have aspect ratios closer to what is common for color-magnitude diagrams (i.e., x:y $\\approx$ 4:6 or so).\n",
    "\n",
    "\n",
    "## Problem 2\n",
    "Investigate what you can learn about a simulated dataset with 4 Gaussian components, with observational errors, using GMM fits with the number of components given by the BIC criterion. \n",
    "\n",
    "Experiment by changing the sample size and measurement errors.\n",
    "\n",
    "\n",
    "## Problem 3 \n",
    "\n",
    "PCA applied to 4-D data for variable stars from the SDSS and LINEAR surveys:\n",
    "\n",
    "- SDSS u-g and g-i colors,\n",
    "\n",
    "- LINEAR variability period (logP) and variability amplitude (A).\n",
    "\n",
    "This is the same data sample as in Problem 2 from Afternoon Activity - Day 7. \n",
    "\n",
    "Read the data, run PCA (with four components), and then\n",
    "\n",
    "a) plot P2 vs. P1 diagram, color-coded using P3 and P4 as in Homework 1. From Homework 1, retain the g-i vs. u-g and amplitude vs. logP diagrams, color-coded by the u-g and g-i colors, for visual comparison.  \n",
    "\n",
    "b) plot the g-i vs. u-g, amplitude vs. logP, P2 vs. P1 and P4 vs. P3 diagrams, color-coded using P1 and P2. \n",
    "\n",
    "Comment what insights, if any, did PCA analysis bring."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Hess diagrams with SDSS data for the Monoceros Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "fieldA = pd.read_csv('data/fieldA.csv')\n",
    "fieldB = pd.read_csv('data/fieldB.csv')\n",
    "\n",
    "### --- EXERCISE --- ###\n",
    "# Add a color column (g-r) in each dataframe:\n",
    "fieldA['g-r'] = \n",
    "fieldB['g-r'] = \n",
    "\n",
    "### --- END OF EXERCISE --- ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.colors import LogNorm, Normalize\n",
    "\n",
    "from scipy.spatial import cKDTree\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "from astroML.datasets import fetch_great_wall\n",
    "\n",
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# This function adjusts matplotlib settings for a uniform feel in the textbook.\n",
    "# Note that with usetex=True, fonts are rendered with LaTeX.  This may\n",
    "# result in an error if LaTeX is not installed on your system.  In that case,\n",
    "# you can set usetex to False.\n",
    "from astroML.plotting import setup_text_plots\n",
    "setup_text_plots(fontsize=10, usetex=True)\n",
    "\n",
    "def make_plots(X, h):\n",
    "    #------------------------------------------------------------\n",
    "    # Create  the grid on which to evaluate the results\n",
    "    Nx = 250\n",
    "    Ny = 250\n",
    "    xmin, xmax = (-0.5, 2)\n",
    "    ymin, ymax = (22, 14)\n",
    "\n",
    "    #------------------------------------------------------------\n",
    "    # Evaluate for several models\n",
    "    Xgrid = np.vstack(map(np.ravel, np.meshgrid(np.linspace(xmin, xmax, Nx),\n",
    "                                                np.linspace(ymin, ymax, Ny)))).T\n",
    "\n",
    "    # Gaussian:\n",
    "    kde1 = KernelDensity(h, kernel='gaussian')\n",
    "    log_dens1 = kde1.fit(X).score_samples(Xgrid)\n",
    "    dens1 = X.shape[0] * np.exp(log_dens1).reshape((Ny, Nx))\n",
    "\n",
    "    ### --- EXERCISE --- ###\n",
    "    # Top-hat\n",
    "    kde2 = \n",
    "    log_dens2 =\n",
    "    dens2 = \n",
    "\n",
    "    # Exponential\n",
    "    kde3 = \n",
    "    log_dens3 = \n",
    "    dens3 =\n",
    "    ### --- END OF EXERCISE --- ###\n",
    "    \n",
    "    #------------------------------------------------------------\n",
    "    # Plot the results\n",
    "    fig = plt.figure(figsize=(10, 15))\n",
    "    fig.subplots_adjust(left=0.12, right=0.95, bottom=0.2, top=0.9,\n",
    "                        hspace=0.1, wspace=0.1)\n",
    "\n",
    "    # First plot: scatter the points\n",
    "    ax1 = plt.subplot(221)\n",
    "    ax1.scatter(X[:, 0], X[:, 1], s=1, lw=0, c='k')\n",
    "    ax1.text(0.95, 0.9, \"input\", ha='right', va='top',\n",
    "             transform=ax1.transAxes,\n",
    "             bbox=dict(boxstyle='round', ec='k', fc='w'))\n",
    "\n",
    "    # Second plot: gaussian kernel\n",
    "    ax2 = plt.subplot(222)\n",
    "    ax2.imshow(dens1, origin='lower', aspect='auto', norm=Normalize(),\n",
    "               extent=(xmin, xmax, ymin, ymax), cmap=plt.cm.binary)\n",
    "    ax2.text(0.95, 0.9, \"Gaussian $(h=%.2f)$\" % h, ha='right', va='top',\n",
    "             transform=ax2.transAxes,\n",
    "             bbox=dict(boxstyle='round', ec='k', fc='w'))\n",
    "\n",
    "    # Third plot: top-hat kernel\n",
    "    ax3 = plt.subplot(223)\n",
    "    ax3.imshow(dens2, origin='lower', aspect='auto', norm=Normalize(),\n",
    "               extent=(xmin, xmax, ymin, ymax), cmap=plt.cm.binary)\n",
    "    ax3.text(0.95, 0.9, \"top-hat $(h=%.2f)$\" % h, ha='right', va='top',\n",
    "             transform=ax3.transAxes,\n",
    "             bbox=dict(boxstyle='round', ec='k', fc='w'))\n",
    "\n",
    "    # Fourth plot: exponential kernel\n",
    "    ax4 = plt.subplot(224)\n",
    "    ax4.imshow(dens3, origin='lower', aspect='auto', norm=Normalize(),\n",
    "               extent=(xmin, xmax, ymin, ymax), cmap=plt.cm.binary)\n",
    "    ax4.text(0.95, 0.9, \"exponential $(h=%.2f)$\" % h, ha='right', va='top',\n",
    "             transform=ax4.transAxes,\n",
    "             bbox=dict(boxstyle='round', ec='k', fc='w'))\n",
    "\n",
    "    for ax in [ax1, ax2, ax3, ax4]:\n",
    "        ax.set_xlim(xmin, xmax)\n",
    "        ax.set_ylim(ymin, ymax)\n",
    "\n",
    "    for ax in [ax1, ax2]:\n",
    "        ax.xaxis.set_major_formatter(plt.NullFormatter())\n",
    "\n",
    "    for ax in [ax3, ax4]:\n",
    "        ax.set_xlabel('$g-r$')\n",
    "\n",
    "    for ax in [ax2, ax4]:\n",
    "        ax.yaxis.set_major_formatter(plt.NullFormatter())\n",
    "\n",
    "    for ax in [ax1, ax3]:\n",
    "        ax.set_ylabel('$g$')\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    return dens1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warning: these steps take a few minutes each!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-19T12:41:06.345007Z",
     "start_time": "2021-07-19T12:41:06.247813Z"
    }
   },
   "outputs": [],
   "source": [
    "### --- EXERCISE --- ###\n",
    "\n",
    "# Run the make_plots function for fieldB. Test different values of h (each person can test one value of h).\n",
    "densityB = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-19T12:41:06.345007Z",
     "start_time": "2021-07-19T12:41:06.247813Z"
    }
   },
   "outputs": [],
   "source": [
    "# Run the make_plots function for fieldA. Test different values of h (each person can test one value of h).\n",
    "densityA = \n",
    "\n",
    "### --- END OF EXERCISE --- ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 7.5))\n",
    "xmin, xmax = (-0.5, 2)\n",
    "ymin, ymax = (22, 14)\n",
    "\n",
    "ax = plt.subplot(131)\n",
    "ax.imshow(densityA, origin='lower', aspect='auto', norm=Normalize(), \n",
    "              extent=(xmin, xmax, ymin, ymax), cmap=plt.cm.binary)\n",
    "\n",
    "ax = plt.subplot(132)\n",
    "ax.imshow(densityB, origin='lower', aspect='auto', norm=Normalize(), \n",
    "              extent=(xmin, xmax, ymin, ymax), cmap=plt.cm.binary)\n",
    "\n",
    "ax = plt.subplot(133)\n",
    "# note renormalization of the second image (fieldA)! \n",
    "ax.imshow(densityB-0.8*densityA, origin='lower', aspect='auto', norm=Normalize(), \n",
    "              extent=(xmin, xmax, ymin, ymax), cmap=plt.cm.binary)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: a dataset with 4 Gaussian components "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gaussian mixture model** is a relatively simple and straightfoward numerical method on obtaining data likelihood function, and expectation maximization algorithm for maximizing the likelihood.  \n",
    "The likelihood of a datum $x_i$ for a Gaussian mixture model is given by  \n",
    "\n",
    "$$p(x_i|\\theta) = \\sum_{j=1}^{M} \\alpha_{j}\\mathcal{N}(\\mu_j, \\sigma_j)$$  \n",
    "\n",
    "where vector of parameters $\\theta$ is estimated from Gaussian with parameters $\\mu_j$ and $\\sigma_j$.   \n",
    "$\\alpha_j$ is the normalization factor for each Gaussian, with $\\sum_{j=1}^{M} \\alpha_{j} = 1$.   \n",
    "M is given.\n",
    "  \n",
    "Both **Akaike information criterion (AIC)** and **Bayesian information criterion (BIC)** are scoring systems for model comparisons in classical statistics dealing with models with different numbers of free parameters.  \n",
    "\n",
    "Specifically, **AIC** is computed as\n",
    "\n",
    "$$AIC \\equiv -2 ln(L^0(M)) + 2k + \\frac{2k(k+1)}{N-k-1}$$  \n",
    "\n",
    "**BIC** is computed as\n",
    "\n",
    "$$BIC \\equiv -2ln[L^0(M)] + k lnN$$  \n",
    "  \n",
    "In this notebook, we are going to apply Gaussian mixture model on a dataset of stellar metallicity. Then we will use AIC, and BIC for model comparisons, and then plot the best scored model in BIC.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-19T13:00:56.794962Z",
     "start_time": "2021-07-19T13:00:56.722291Z"
    }
   },
   "outputs": [],
   "source": [
    "### Modeled after astroML book figure 4.2  \n",
    "from sklearn.mixture import GaussianMixture as GMM\n",
    "setup_text_plots(fontsize=18, usetex=True)\n",
    "\n",
    "def doGMM(Ndata, noiseStd=0.00000001):\n",
    "    #------------------------------------------------------------\n",
    "    # Set up the dataset by drawing samples from Gaussians.\n",
    "    #  We'll use scikit-learn's Gaussian Mixture Model to sample\n",
    "    #  data from a mixture of 1-D Gaussians.  The usual way of using\n",
    "    #  this involves fitting the mixture to data: we'll see that\n",
    "    #  below.  Here we'll set the internal means, covariances,\n",
    "    #  and weights by-hand and then generate a sample of requested \n",
    "    #  size Ndata. \n",
    "    N1 = int(0.2*Ndata)\n",
    "    N2 = int(0.3*Ndata)\n",
    "    N3 = int(0.2*Ndata)\n",
    "    N4 = Ndata - (N1+N2+N3)\n",
    "    random_state = np.random.RandomState(seed=1)\n",
    "    X = np.concatenate([random_state.normal(-4, 1.1, N1),\n",
    "                    random_state.normal(-1, 1.0, N2),\n",
    "                    random_state.normal(0.9, 0.7, N3),\n",
    "                    random_state.normal(3, 1.0, N4)]).reshape(-1, 1)\n",
    "    X = np.random.normal(X, noiseStd)\n",
    "\n",
    "    #------------------------------------------------------------\n",
    "    # Learn (train) the best-fit GMM models\n",
    "    # Here we'll use GMM in the standard way: the fit() method\n",
    "    # uses an Expectation-Maximization approach to find the best\n",
    "    # mixture of Gaussians for the data\n",
    "\n",
    "    # fit models with 1-10 components\n",
    "    \n",
    "    N = np.arange(1,11)\n",
    "    models = [None for i in range(len(N))]\n",
    "    \n",
    "    ### --- EXERCISE --- ###\n",
    "    \n",
    "    # Fit GMM for each N:\n",
    "    for i in range(len(N)):\n",
    "        models[i] = \n",
    "    \n",
    "    # Compute the BIC for each model in models[i]. Hint: you do not need to write down the BIC equation\n",
    "    # The BIC value is already calculated when you fit GMM using sklearn. \n",
    "    # (check https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html)\n",
    "    # Use list comprehension: BIC = [... for m in models]. This should be an array.\n",
    "    \n",
    "    BIC = \n",
    "    \n",
    "    # Get the model index with minimum BIC (use np.argmin). This should be a scalar:\n",
    "    \n",
    "    i_best = \n",
    "    \n",
    "    ### --- END OF EXERCISE --- ###\n",
    "    \n",
    "    \n",
    "\n",
    "    #------------------------------------------------------------\n",
    "    # Plot the results\n",
    "    #  We'll use two panels:\n",
    "    #   1) data + best-fit mixture\n",
    "    #   2) BIC vs number of components \n",
    "\n",
    "    fig = plt.figure(figsize=(12, 7))\n",
    "    fig.subplots_adjust(left=0.12, right=0.97,\n",
    "                    bottom=0.21, top=0.9, wspace=0.5)\n",
    "\n",
    "\n",
    "    # plot 1: data + best-fit mixture\n",
    "    ax = fig.add_subplot(121)\n",
    "    M_best = models[i_best]\n",
    "\n",
    "    x = np.linspace(-10, 10, 1000)\n",
    "    logprob = M_best.score_samples(x.reshape(-1, 1))\n",
    "    responsibilities = M_best.predict_proba(x.reshape(-1, 1))\n",
    "\n",
    "    pdf = np.exp(logprob)\n",
    "    pdf_individual = responsibilities * pdf[:, np.newaxis]\n",
    "\n",
    "    ax.hist(X, 'auto', density=True, histtype='stepfilled', alpha=0.4)\n",
    "    ax.plot(x, pdf, '-k')\n",
    "    ax.plot(x, pdf_individual, '--k')\n",
    "    ax.text(0.04, 0.96, \"Best-fit Mixture\",\n",
    "        ha='left', va='top', transform=ax.transAxes)\n",
    "    ax.set_xlabel('$x$')\n",
    "    ax.set_ylabel('$p(x)$')\n",
    "\n",
    "    # plot 2: BIC\n",
    "    ax = fig.add_subplot(122)\n",
    "    ax.plot(N, BIC, '--k')\n",
    "    ax.set_xlabel('n. components')\n",
    "    ax.set_ylabel('BIC')\n",
    "\n",
    "    ibest = np.argmin(BIC)\n",
    "    print(BIC[ibest-1]-BIC[ibest], BIC[ibest], BIC[ibest+1]-BIC[ibest])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### -- EXERCISE -- ####\n",
    "\n",
    "# Call doGMM(). Try a huge sample (such as 100000). It takes about 4 min for 1 million points.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now call doGMM() with a smaller sample (such as 300). Feel free to test other values.\n",
    "\n",
    "\n",
    "### -- END OF EXERCISE -- ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  How do we interpret BIC? \n",
    "\n",
    "The figure above (right panel) shows that the 3-component model has a BIC lower\n",
    "by about 15 than the 4-component model. Is this sufficient evidence to claim \n",
    "that the former is really really better choice than the latter? \n",
    "\n",
    "As you may recall,  \n",
    "\n",
    "**How do we interpret the values of the odds ratio in practice? **\n",
    "Jeffreys proposed a five-step scale for interpreting the odds ratio, where $O_{21} > 10$ represents “strong” evidence in favor of $M_2$ ($M_2$ is ten times more probable than $M_1$), and $O_{21} > 100$ is “decisive” evidence ($M_2$ is one hundred times more probable than $M_1$). When $O_{21} < 3$, the evidence is “not worth more than a bare mention.”\n",
    "\n",
    "Given the relationship between the BIC and $O_{21},$ \n",
    "$$    \\Delta BIC \\equiv BIC_2 - BIC_1 = -2\\,\\ln(O_{21}) $$ \n",
    "we have that $O_{21}=10$ corresponds to $\\Delta BIC= -4.6$ and $O_{21}=100$ corresponds to $\\Delta BIC = -9.2$.\n",
    "For completeness, $|\\Delta BIC|<2$ is “not worth more than a bare mention.”\n",
    "\n",
    "Therefore, **$|\\Delta BIC| = 5$ corresponds to “strong” evidence and $|\\Delta BIC| = 10$ to “decisive” evidence.**\n",
    "\n",
    "Another way to interpret $|\\Delta BIC| = 15$ is to say that the model with 3 components is $exp(15/2) \\approx 2000$ times more probable than the model with 4 components.\n",
    "\n",
    "We can conclude that the figure shows **decisive** evidence in favor of the 3-component model. That is,\n",
    "you should **not** argue that there are 4 peaks in the left panel! Altough we **did** draw that dataset\n",
    "from a model with 4 components, a sample of 300 values is insufficient to \n",
    "*discover* more than 3 components!\n",
    "\n",
    "What if we increase the sample size? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time doGMM(5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion Large sample size helps! I love surveys, I love LSST!\n",
    "\n",
    "But one more thing: let's convolve our data with an arbitrary Gaussian to simulate measurement errors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time doGMM(5000,1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion** If you badly \"blur\" your sample, of course you will not see the fine\n",
    "structure (i.e. many components)! \n",
    "\n",
    "Therefore, even with large samples, you still need to have precise and well calibrated \n",
    "measurements! Another reason to love LSST!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Principal Component Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First obtain LINEAR data for 6,146 stars using astroML built-in function: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astroML.datasets import fetch_LINEAR_geneva\n",
    "data = fetch_LINEAR_geneva()\n",
    "# N.B. this is a 7-dimensional dataset\n",
    "ug = data['ug']\n",
    "gi = data['gi']\n",
    "logP = data['logP']\n",
    "A = data['Ampl']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few helper routines before starting with the PCA..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def black_bg_subplot(*args, **kwargs):\n",
    "# copied from http://www.astroml.org/book_figures/chapter1/fig_moving_objects_multicolor.html\n",
    "    \"\"\"Create a subplot with black background\"\"\"\n",
    "    kwargs['facecolor'] = 'k'\n",
    "    ax = plt.subplot(*args, **kwargs)\n",
    "\n",
    "    # set ticks and labels to white\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_color('w')\n",
    "\n",
    "    for tick in ax.xaxis.get_major_ticks() + ax.yaxis.get_major_ticks():\n",
    "        for child in tick.get_children():\n",
    "            child.set_color('w')\n",
    "\n",
    "    return ax\n",
    "\n",
    "def compute_color2(vec1, vec2, kG=5.0, aG=0.8, kB=2.0, aB=1.2):\n",
    "# adopted from http://www.astroml.org/book_figures/chapter1/fig_moving_objects_multicolor.html\n",
    "    \"\"\"\n",
    "    Compute the scatter-plot color using code adapted from astroML\n",
    "    http://www.astroml.org/book_figures/chapter1/fig_moving_objects_multicolor.html\n",
    "    \"\"\"\n",
    "    # define the base color scalings\n",
    "    # fudge factors aG and aB control the \"redness\" and \"greenness\"\n",
    "    # fudge factors kG and kB control dynamic range of color coding\n",
    "    # both a and k factors need some experimenting for the best results\n",
    "    R = np.ones_like(vec1)\n",
    "    G = aG * 10**(kG*vec1) \n",
    "    B = aB * 10**(kB*vec2) \n",
    "\n",
    "    # normalize color of each point to its maximum component\n",
    "    RGB = np.vstack([R, G, B])\n",
    "    RGB /= RGB.max(0)\n",
    "\n",
    "    # return an array of RGB colors, which is shape (n_points, 3)\n",
    "    return RGB.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first plot diagrams as in HW #1\n",
    "ugNorm = (ug-1.5)/(3.0-0.0)\n",
    "giNorm = (gi-0.5)/(2.5+1.0)\n",
    "color = compute_color2(ugNorm, giNorm, 5.0, 0.8, 2.0, 1.2)\n",
    "\n",
    "# set up the plot\n",
    "fig = plt.figure(figsize=(12, 9), facecolor='k')\n",
    "fig.subplots_adjust(left=0.1, right=0.95, wspace=0.2,\n",
    "                    bottom=0.1, top=0.93)\n",
    "\n",
    "# plot the color-color plot\n",
    "ax1 = black_bg_subplot(221)\n",
    "ax1.scatter(ug, gi, c=color, s=0.9, lw=0)\n",
    "ax1.set_xlim(0.0, 3.0)\n",
    "ax1.set_ylim(-1.0, 2.5)\n",
    "ax1.set_xlabel(r'${\\rm u-g}$', color='w')\n",
    "ax1.set_ylabel(r'${\\rm g-i}$', color='w')\n",
    "\n",
    "# plot the A vs. logP plot\n",
    "ax2 = black_bg_subplot(222)\n",
    "ax2.scatter(logP, A, c=color, s=0.9, lw=0)\n",
    "ax2.set_xlim(-1.5, 0.5)\n",
    "ax2.set_ylim(0.0, 1.5)\n",
    "ax2.set_xlabel(r'${\\rm logP \\, (days)}$', color='w')\n",
    "ax2.set_ylabel(r'${\\rm Amplitude \\,\\, (mag)}$', color='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### note 4-dimensional color coding: the symbol color is determined by the position in the \n",
    "left panel and used in the right panel\n",
    "\n",
    "\n",
    "## And now do the PCA analysis..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "X = np.column_stack((ug, gi, logP, A))\n",
    "\n",
    "# Call PCA with 4 components and whiten = False:\n",
    "pca = \n",
    "\n",
    "# Call pca.fit() to fit PCA \n",
    "# (Hint: check examples https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html):\n",
    "\n",
    "# Use .transform() to compute the subspace projection of X:\n",
    "comp = \n",
    "\n",
    "\n",
    "mean = pca.mean_ # length 4 mean of the data\n",
    "components = pca.components_ # 4 x 4 matrix of components\n",
    "var = pca.explained_variance_ # the length 4 array of eigenvalues\n",
    "var_rat = pca.explained_variance_ratio_\n",
    "\n",
    "# and now generate principal axes values\n",
    "P1 = comp[:,0]\n",
    "P2 = comp[:,1]\n",
    "P3 = comp[:,2]\n",
    "P4 = comp[:,3]\n",
    "print(np.mean(P1), np.min(P1), np.max(P1))\n",
    "print(np.mean(P2), np.min(P2), np.max(P2))\n",
    "print(np.mean(P3), np.min(P3), np.max(P3))\n",
    "print(np.mean(P4), np.min(P4), np.max(P4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now compute RGB color based on P3 and P4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## and now use principal coordinates \n",
    "P3norm = P3/3.0\n",
    "P4norm = P4/3.0 \n",
    "# for the last four parameters, see compute_color2 above\n",
    "colorP = compute_color2(P3norm, P4norm, 5.0, 0.8, 2.0, 1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the plot\n",
    "fig = plt.figure(figsize=(12, 9), facecolor='k')\n",
    "fig.subplots_adjust(left=0.1, right=0.95, wspace=0.2,\n",
    "                    bottom=0.1, top=0.93)\n",
    "\n",
    "# plot the color-color plot\n",
    "ax1 = black_bg_subplot(223)\n",
    "ax1.scatter(P1, P2, c=colorP, s=0.9, lw=0)\n",
    "ax1.set_xlim(-1.0, 1.0)\n",
    "ax1.set_ylim(-1.5, 2.5)\n",
    "ax1.set_xlabel(r'${\\rm P_1}$', color='w')\n",
    "ax1.set_ylabel(r'${\\rm P_2}$', color='w')\n",
    "\n",
    "# plot the A vs. logP plot\n",
    "ax2 = black_bg_subplot(224)\n",
    "ax2.scatter(P3, P4, c=colorP, s=0.9, lw=0)\n",
    "ax2.set_xlim(-1.5, 1.5)\n",
    "ax2.set_ylim(-1.5, 1.5)\n",
    "ax2.set_xlabel(r'${\\rm P_3}$', color='w')\n",
    "ax2.set_ylabel(r'${\\rm P_4}$', color='w')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## now use principal coordinates P1 and P2 to define color\n",
    "P1norm = P1/3.0\n",
    "P2norm = P2/3.0 \n",
    "# for the last four parameters, see compute_color2 above\n",
    "colorP12 = compute_color2(P1norm, P2norm, 5.0, 0.8, 2.0, 1.2)\n",
    "\n",
    "# set up the plot\n",
    "fig = plt.figure(figsize=(12, 9), facecolor='k')\n",
    "fig.subplots_adjust(left=0.1, right=0.95, wspace=0.2,\n",
    "                    bottom=0.1, top=0.93)\n",
    "\n",
    "# plot the color-color plot\n",
    "ax1 = black_bg_subplot(221)\n",
    "ax1.scatter(ug, gi, c=colorP12, s=0.9, lw=0)\n",
    "ax1.set_xlim(0.0, 3.0)\n",
    "ax1.set_ylim(-1.0, 2.5)\n",
    "ax1.set_xlabel(r'${\\rm u-g}$', color='w')\n",
    "ax1.set_ylabel(r'${\\rm g-i}$', color='w')\n",
    "\n",
    "# plot the A vs. logP plot\n",
    "ax2 = black_bg_subplot(222)\n",
    "ax2.scatter(logP, A, c=colorP12, s=0.9, lw=0)\n",
    "ax2.set_xlim(-1.5, 0.5)\n",
    "ax2.set_ylim(0.0, 1.5)\n",
    "ax2.set_xlabel(r'${\\rm logP \\, (days)}$', color='w')\n",
    "ax2.set_ylabel(r'${\\rm Amplitude \\,\\, (mag)}$', color='w')\n",
    "\n",
    "# plot the color-color plot\n",
    "ax1 = black_bg_subplot(223)\n",
    "ax1.scatter(P1, P2, c=colorP12, s=0.9, lw=0)\n",
    "ax1.set_xlim(-1.0, 1.0)\n",
    "ax1.set_ylim(-1.5, 2.5)\n",
    "ax1.set_xlabel(r'${\\rm P_1}$', color='w')\n",
    "ax1.set_ylabel(r'${\\rm P_2}$', color='w')\n",
    "\n",
    "# plot the A vs. logP plot\n",
    "ax2 = black_bg_subplot(224)\n",
    "ax2.scatter(P3, P4, c=colorP12, s=0.9, lw=0)\n",
    "ax2.set_xlim(-1.5, 1.5)\n",
    "ax2.set_ylim(-1.5, 1.5)\n",
    "ax2.set_xlabel(r'${\\rm P_3}$', color='w')\n",
    "ax2.set_ylabel(r'${\\rm P_4}$', color='w')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## what do you conclude? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
