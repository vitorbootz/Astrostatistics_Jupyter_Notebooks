{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Afternoon Activity - Day 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This notebook was developed by [Zeljko Ivezic](http://faculty.washington.edu/ivezic/) for the 2021 data science class at the University of Sao Paulo and it is available from [github](https://github.com/ivezic/SaoPaulo2021/blob/main/notebooks/Activity9.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Problem 1: clustering of orbital data for asteroids\n",
    "\n",
    "Asteroids are clustered in orbital parameter space (6 dimensions\n",
    "but here we will consider two: semi-major axis and inclination)\n",
    "and these clusters, known as families, are believed to be remnants of larger asteroids destroyed in collisions. Typically, families have\n",
    "uniform colors, for more details see [Parker et al. 2008.](\n",
    "http://faculty.washington.edu/ivezic/Publications/parker.pdf)\n",
    "\n",
    "Using the Parker et al. dataset, available from astroML, apply \n",
    "Gaussian Mixture Model and the Minimum Spanning Tree model and\n",
    "compare their clustering results.\n",
    "\n",
    "\n",
    "\n",
    "## Problem 2: supervised classification of periodic variable stars in 4-D space of colors and lightcurve parameters\n",
    "\n",
    "Let's apply a number of classification methods to 4-D data for periodic variable stars from the SDSS and LINEAR surveys:\n",
    "\n",
    "-- SDSS u-g and g-i colors,\n",
    "\n",
    "-- LINEAR variability period (logP) and variability amplitude (A).\n",
    "\n",
    "We also have labels (light curve type \n",
    "[determined by domain experts](http://faculty.washington.edu/ivezic/Publications/Palaversa2013LINEAR.pdf)\n",
    "and thus we are **implementing supervised classification**. \n",
    "\n",
    "The question we are asking in this exercise is: \"How well can we separate RR Lyrae stars from other periodic variable stars using this 4-D information?\"\n",
    "\n",
    "This is the same data sample as in Problem 2 from Afternoon Activity - Day 7 and Problem 3 from Afternoon Activity - Day 8. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-19T19:03:22.141686Z",
     "start_time": "2021-07-19T19:03:22.135180Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from scipy import optimize\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.mixture import GaussianMixture as GMM\n",
    "from astroML.plotting.mcmc import convert_to_stdev\n",
    "from astroML.plotting import setup_text_plots\n",
    "from astroML.datasets import fetch_moving_objects\n",
    "from astroML.plotting.tools import draw_ellipse\n",
    "from astroML.clustering import HierarchicalClustering, get_graph_segments\n",
    "setup_text_plots(fontsize=8, usetex=True)\n",
    "# random seed \n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: clustering with Gaussian Mixture Model and the Minimum Spanning Tree model for asteroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### For completeness, we expect structure like this:\n",
    "\n",
    "![Parker et al. asteroid families](http://faculty.washington.edu/ivezic/sdssmoc/MOC4_population_labels.jpg) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-19T19:05:58.269758Z",
     "start_time": "2021-07-19T19:05:57.924314Z"
    }
   },
   "outputs": [],
   "source": [
    "# Fetch data and extract the desired quantities\n",
    "dataAll = fetch_moving_objects(Parker2008_cuts=True)\n",
    "data = dataAll[1:]\n",
    "a = data['aprime'] # semi-major axis\n",
    "sini = data['sin_iprime'] # inclination\n",
    "acolor = data['mag_a']\n",
    "izcolor = data['mag_i'] - data['mag_z']\n",
    "\n",
    "# Creating a numpy matrix with the features we will use to fit the models:\n",
    "X = np.vstack([a, sini]).T \n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Plot \n",
    "xmin, xmax = (2.0, 3.3)\n",
    "ymin, ymax = (0.0, 0.33)\n",
    "fig = plt.figure(figsize=(12, 12))\n",
    "fig.subplots_adjust(hspace=0, left=0.1, right=0.95, bottom=0.1, top=0.9)\n",
    "\n",
    "ax = fig.add_subplot(311)\n",
    "ax.scatter(X[:, 0], X[:, 1], s=1, lw=0.5, c='k')\n",
    "ax.set_xlim(xmin, xmax)\n",
    "ax.set_ylim(ymin, ymax)\n",
    "ax.set_xlabel('a (AU)')\n",
    "ax.set_ylabel('sin(i)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-19T19:05:57.904123Z",
     "start_time": "2021-07-19T19:04:03.111835Z"
    }
   },
   "outputs": [],
   "source": [
    "#------------------------------------------------------------\n",
    "# Compute GMM models & AIC/BIC\n",
    "\n",
    "def compute_GMM(N, covariance_type='full', max_iter=100):\n",
    "    models = [None for n in N]\n",
    "    for i in range(len(N)):\n",
    "        \n",
    "        ### --- EXERCISE --- ###\n",
    "        \n",
    "        # Call GMM() receiving number of components N[i], max_iter, and covariance_type:\n",
    "        models[i] = \n",
    "        \n",
    "        # Use .fit() to fit GMM: \n",
    "        \n",
    "        \n",
    "        ### --- END OF EXERCISE --- ###\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-19T19:05:57.904123Z",
     "start_time": "2021-07-19T19:04:03.111835Z"
    }
   },
   "outputs": [],
   "source": [
    "N = np.arange(1, 60) # number of components\n",
    "\n",
    "### --- EXERCISE --- ###\n",
    "# Call the function compute_GMM:\n",
    "models = \n",
    "\n",
    "### --- END OF EXERCISE --- ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-19T19:05:57.904123Z",
     "start_time": "2021-07-19T19:04:03.111835Z"
    }
   },
   "outputs": [],
   "source": [
    "### --- EXERCISE --- ###\n",
    "\n",
    "# Compute the BIC and AIC for each model in models[i]. Hint: you do not need to write down the BIC and AIC equations\n",
    "# The BIC and AIC value are already calculated when you fit GMM using sklearn. \n",
    "# (check https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html)\n",
    "# Use list comprehension: BIC = [... for m in models]\n",
    "\n",
    "AIC = \n",
    "BIC = \n",
    "\n",
    "# Get the model index with minimum BIC (use np.argmin):\n",
    "i_best = \n",
    "\n",
    "### --- END OF EXERCISE --- ###\n",
    "\n",
    "gmm_best = models[i_best]\n",
    "\n",
    "\n",
    "print(\"best fit converged:\", gmm_best.converged_)\n",
    "print(\"BIC: n_components =  %i\" % N[i_best])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-19T19:19:10.834717Z",
     "start_time": "2021-07-19T19:19:09.000536Z"
    }
   },
   "outputs": [],
   "source": [
    "#------------------------------------------------------------\n",
    "# Plot the results\n",
    "fig = plt.figure(figsize=(12, 12))\n",
    "fig.subplots_adjust(wspace=0.45, bottom=0.25, top=0.9, left=0.1, right=0.97)\n",
    "\n",
    "# plot AIC/BIC\n",
    "ax = fig.add_subplot(211)\n",
    "ax.plot(N, AIC, '-k', label='AIC')\n",
    "ax.plot(N, BIC, ':k', label='BIC')\n",
    "ax.legend(loc=1)\n",
    "ax.set_xlabel('N components')\n",
    "plt.setp(ax.get_yticklabels(), fontsize=7)\n",
    "\n",
    "# plot best configurations for AIC and BIC\n",
    "ax = fig.add_subplot(212)\n",
    "\n",
    "ax.scatter(a, sini, c='red', s=1, lw=0)\n",
    "\n",
    "ind = np.argsort(gmm_best.weights_)[::-1]\n",
    "for cnt, i in enumerate(ind[:N[i_best]]):\n",
    "    mu = gmm_best.means_[i]\n",
    "    C = gmm_best.covariances_[i]\n",
    "    w = gmm_best.weights_[i]\n",
    "    print(cnt, i, w)\n",
    "    # the 20 most numerous clusters in blue\n",
    "    if (cnt<20):\n",
    "        draw_ellipse(mu, C, scales=[1.0], ax=ax, fc='none', lw=1.0, ec='blue')\n",
    "    else:\n",
    "        draw_ellipse(mu, C, scales=[1.0], ax=ax, fc='none', lw=0.5, ec='k')\n",
    " \n",
    "ax.plot([2.5, 2.5], [-0.02, 0.3], '--k')\n",
    "ax.plot([2.82, 2.82], [-0.02, 0.3], '--k')\n",
    "\n",
    "ax.set_xlim(2.0, 3.3)\n",
    "ax.set_ylim(-0.02, 0.3)\n",
    "\n",
    "ax.set_xlabel('a (AU)')\n",
    "ax.set_ylabel('sin(i)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What do you conclude about these GMM results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-19T19:19:27.267517Z",
     "start_time": "2021-07-19T19:19:26.728537Z"
    }
   },
   "outputs": [],
   "source": [
    "#------------------------------------------------------------\n",
    "# Compute the Minimum Spanning Tree clustering model\n",
    "n_neighbors = 10\n",
    "edge_cutoff = 0.9\n",
    "cluster_cutoff = 10\n",
    "\n",
    "### --- EXERCISE --- ###\n",
    "\n",
    "# Call HierarchicalClustering with the specifications of hyperparameters listed above:\n",
    "\n",
    "model = \n",
    "\n",
    "# Use .fit() to fit HierarchicalClustering:\n",
    "\n",
    "### --- END OF EXERCISE --- ###\n",
    "\n",
    "print(\" scale: %2g\" % np.percentile(model.full_tree_.data,\n",
    "                                        100 * edge_cutoff))\n",
    "n_components = model.n_components_\n",
    "labels = model.labels_\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Get the x, y coordinates of the beginning and end of each line segment\n",
    "T_x, T_y = get_graph_segments(model.X_train_,\n",
    "                              model.full_tree_)\n",
    "T_trunc_x, T_trunc_y = get_graph_segments(model.X_train_,\n",
    "                                          model.cluster_graph_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-19T19:28:02.462445Z",
     "start_time": "2021-07-19T19:26:54.836874Z"
    }
   },
   "outputs": [],
   "source": [
    "#------------------------------------------------------------\n",
    "# Plot the results\n",
    "xmin, xmax = (2.0, 3.3)\n",
    "ymin, ymax = (0.0, 0.33)\n",
    "fig = plt.figure(figsize=(12, 12))\n",
    "fig.subplots_adjust(hspace=0, left=0.1, right=0.95, bottom=0.1, top=0.9)\n",
    "\n",
    "ax = fig.add_subplot(311)\n",
    "ax.scatter(X[:, 0], X[:, 1], s=1, lw=0.5, c='k')\n",
    "ax.set_xlim(xmin, xmax)\n",
    "ax.set_ylim(ymin, ymax)\n",
    "ax.xaxis.set_major_formatter(plt.NullFormatter())\n",
    "ax.set_ylabel('sin(i)')\n",
    "\n",
    "ax = fig.add_subplot(312)\n",
    "ax.scatter(X[:, 0], X[:, 1], s=1, lw=0.8, c='k')\n",
    "ax.plot(T_x, T_y, c='red', lw=0.8)\n",
    "ax.set_xlim(xmin, xmax)\n",
    "ax.set_ylim(ymin, ymax)\n",
    "ax.xaxis.set_major_formatter(plt.NullFormatter())\n",
    "ax.set_ylabel('sin(i)')\n",
    "\n",
    "ax = fig.add_subplot(313)\n",
    "ax.scatter(X[:, 0], X[:, 1], s=1, lw=0.8, c='k')\n",
    "ax.plot(T_trunc_x, T_trunc_y, c='red', lw=0.8)\n",
    "\n",
    "ax.set_xlim(xmin, xmax)\n",
    "ax.set_ylim(ymin, ymax)\n",
    "ax.set_xlabel('a (AU)')\n",
    "ax.set_ylabel('sin(i)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What do you conclude about these results? How do they compare to GMM results? \n",
    "\n",
    "#### Note: we could now, on per object basis, compare the two classifications into background and clustered objects and somehow (depending on application) combine them into the final classification. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Problem 2: supervised classification of periodic variable stars in 7-D space of colors and lightcurve parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-19T19:28:25.648312Z",
     "start_time": "2021-07-19T19:28:25.593895Z"
    }
   },
   "outputs": [],
   "source": [
    "from astroML.datasets import fetch_LINEAR_geneva\n",
    "from astroML.utils import split_samples\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-19T19:28:29.792148Z",
     "start_time": "2021-07-19T19:28:29.744922Z"
    }
   },
   "outputs": [],
   "source": [
    "# Fetch data and extract the desired quantities\n",
    "data = fetch_LINEAR_geneva()\n",
    "# this is a 7-dimensional dataset\n",
    "attributes = [('gi', 'logP', 'ug', 'iK', 'JK', 'amp', 'skew')]\n",
    "ug = data['ug']\n",
    "gi = data['gi']\n",
    "logP = data['logP']\n",
    "amp = data['Ampl'] \n",
    "skew = data['skew'] \n",
    "labels = data['LCtype']\n",
    "# for historic reasons, type 3 is missing (and/or called 6)\n",
    "labels[labels == 6] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-19T19:28:48.908065Z",
     "start_time": "2021-07-19T19:28:48.892477Z"
    }
   },
   "outputs": [],
   "source": [
    "# enforce *nicer* color choices\n",
    "symbcolor = np.full(np.size(labels), 'magenta')\n",
    "symbcolor[labels == 1] = 'red'\n",
    "symbcolor[labels == 2] = 'blue'\n",
    "symbcolor[labels == 3] = 'black'\n",
    "symbcolor[labels == 4] = 'magenta'\n",
    "symbcolor[labels == 5] = 'green'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-19T19:28:54.696287Z",
     "start_time": "2021-07-19T19:28:52.094112Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot classes \n",
    "fig = plt.figure(figsize=(9, 9))\n",
    "fig.subplots_adjust(wspace=0.2, hspace=0.2, left=0.1, right=0.95, bottom=0.1, top=0.9)\n",
    "\n",
    "ax = fig.add_subplot(221)\n",
    "xmin, xmax = (0.0, 3.0)\n",
    "ymin, ymax = (-1.0, 3.0)\n",
    "ax.scatter(ug, gi, s=1, lw=1.0, ec=symbcolor)\n",
    "ax.set_xlim(xmin, xmax)\n",
    "ax.set_ylim(ymin, ymax)\n",
    "ax.set_xlabel('u-g', fontsize=16)\n",
    "ax.set_ylabel('g-i', fontsize=16)\n",
    "\n",
    "ax = fig.add_subplot(222)\n",
    "xmin, xmax = (-1.0, 3.0)\n",
    "ymin, ymax = (-1.6, 1.0)\n",
    "ax.scatter(gi, logP, s=1, lw=1.0, ec=symbcolor)\n",
    "ax.set_xlim(xmin, xmax)\n",
    "ax.set_ylim(ymin, ymax)\n",
    "ax.set_xlabel('g-i', fontsize=16)\n",
    "ax.set_ylabel('log(P/day)', fontsize=16)\n",
    "\n",
    "ax = fig.add_subplot(223)\n",
    "xmin, xmax = (-1.6, 1.0)\n",
    "ymin, ymax = (0.0, 1.5)\n",
    "ax.scatter(logP, amp, s=1, lw=0.5, ec=symbcolor)\n",
    "ax.set_xlim(xmin, xmax)\n",
    "ax.set_ylim(ymin, ymax)\n",
    "ax.set_xlabel('log(P/day)', fontsize=16)\n",
    "ax.set_ylabel('Amplitude', fontsize=16)\n",
    "\n",
    "ax = fig.add_subplot(224)\n",
    "xmin, xmax = (-1.0, 1.0)\n",
    "ymin, ymax = (-3, 12.0)\n",
    "ax.scatter(logP, skew, s=1, lw=1.0, ec=symbcolor)\n",
    "ax.set_xlim(xmin, xmax)\n",
    "ax.set_ylim(ymin, ymax)\n",
    "ax.set_xlabel('log(P/day)', fontsize=16)\n",
    "ax.set_ylabel('skewness', fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-19T19:29:00.629691Z",
     "start_time": "2021-07-19T19:29:00.565548Z"
    }
   },
   "outputs": [],
   "source": [
    "Ndim = 4\n",
    "Ndata = labels.size\n",
    "X = np.empty((Ndata, Ndim), dtype=float)\n",
    "X[:Ndata, 0] = ug\n",
    "X[:Ndata, 1] = gi\n",
    "if (Ndim > 2):\n",
    "    X[:Ndata, 2] = logP\n",
    "    X[:Ndata, 3] = amp\n",
    "\n",
    "y = np.zeros(Ndata, dtype=int)\n",
    "y[:Ndata] = labels\n",
    "# set ab and c type RR Lyrae to 1, other to 0\n",
    "y[y == 2] = 1\n",
    "y[y != 1] = 0\n",
    "\n",
    "# split into training and test sets\n",
    "(X_train, X_test), (y_train, y_test) = split_samples(X, y, [0.9, 0.1], random_state=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-19T20:04:27.746599Z",
     "start_time": "2021-07-19T20:04:27.711357Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "class GMMBayes(GaussianNB):\n",
    "    \"\"\"GaussianMixture Bayes Classifier\n",
    "\n",
    "    This is a generalization to the Naive Bayes classifier: rather than\n",
    "    modeling the distribution of each class with axis-aligned gaussians,\n",
    "    GMMBayes models the distribution of each class with mixtures of\n",
    "    gaussians.  This can lead to better classification in some cases.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_components : int or list\n",
    "        number of components to use in the GaussianMixture. If specified as\n",
    "        a list, it must match the number of class labels. Default is 1.\n",
    "    **kwargs : dict, optional\n",
    "        other keywords are passed directly to GaussianMixture\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_components=1, **kwargs):\n",
    "        self.n_components = np.atleast_1d(n_components)\n",
    "        self.kwargs = kwargs\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = np.asarray(X)\n",
    "        y = np.asarray(y)\n",
    "\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        if n_samples != y.shape[0]:\n",
    "            raise ValueError(\"X and y have incompatible shapes\")\n",
    "\n",
    "        self.classes_ = np.unique(y)\n",
    "        self.classes_.sort()\n",
    "        unique_y = self.classes_\n",
    "\n",
    "        n_classes = unique_y.shape[0]\n",
    "\n",
    "        if self.n_components.size not in (1, len(unique_y)):\n",
    "            raise ValueError(\"n_components must be compatible with \"\n",
    "                             \"the number of classes\")\n",
    "\n",
    "        self.gmms_ = [None for i in range(n_classes)]\n",
    "        self.class_prior_ = np.zeros(n_classes)\n",
    "\n",
    "        n_comp = np.zeros(len(self.classes_), dtype=int) + self.n_components\n",
    "\n",
    "        for i, y_i in enumerate(unique_y):\n",
    "            if n_comp[i] > X[y == y_i].shape[0]:\n",
    "                warnstr = (\"Expected n_samples >= n_components but got \"\n",
    "                           \"n_samples={0}, n_components={1}, \"\n",
    "                           \"n_components set to {0}.\")\n",
    "                warnings.warn(warnstr.format(X[y == y_i].shape[0], n_comp[i]))\n",
    "                n_comp[i] = y_i\n",
    "            \n",
    "            self.gmms_[i] = GaussianMixture(n_comp[i], **self.kwargs).fit(X[y == y_i])\n",
    "            self.class_prior_[i] = float(np.sum(y == y_i)) / n_samples\n",
    "\n",
    "        return self\n",
    "\n",
    "    def _joint_log_likelihood(self, X):\n",
    "\n",
    "        X = np.asarray(np.atleast_2d(X))\n",
    "        logprobs = np.array([g.score_samples(X) for g in self.gmms_]).T\n",
    "        return logprobs + np.log(self.class_prior_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-19T20:04:28.828046Z",
     "start_time": "2021-07-19T20:04:28.438591Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compute fits for all the classifiers\n",
    "def compute_results(*args):\n",
    "    names = []\n",
    "    probs = []\n",
    "\n",
    "    for classifier, kwargs in args:\n",
    "        print(classifier.__name__)\n",
    "        model = classifier(**kwargs)\n",
    "        ### --- EXERCISE --- ###\n",
    "        \n",
    "        # Call .fit() to fit the classifier with X and y from the training sample.\n",
    "        # Hint: check the arguments from the module fit() inside the GMMBayes class in the last cell\n",
    "        # (note that self is not an argument to be called outside the class definition)\n",
    "\n",
    "        \n",
    "        # Use .predict_proba() to calculate probability predictions for the testing sample:\n",
    "        y_prob = \n",
    "\n",
    "        ### --- END OF EXERCISE --- ###\n",
    "        \n",
    "        names.append(classifier.__name__)\n",
    "        probs.append(y_prob[:, 1])\n",
    "\n",
    "    return names, probs\n",
    "\n",
    "LRclass_weight = dict([(i, np.sum(y_train == i)) for i in (0, 1)])\n",
    "\n",
    "names, probs = compute_results((GaussianNB, {}),\n",
    "                               (QDA, {}),\n",
    "                               (LogisticRegression,\n",
    "                                dict(class_weight=LRclass_weight)),\n",
    "                               (KNeighborsClassifier,\n",
    "                                dict(n_neighbors=10)),\n",
    "                               (DecisionTreeClassifier,\n",
    "                                dict(random_state=0, max_depth=12,\n",
    "                                     criterion='entropy')),\n",
    "                               (GMMBayes, dict(n_components=3, tol=1E-5,\n",
    "                                               covariance_type='full')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-19T20:04:35.847726Z",
     "start_time": "2021-07-19T20:04:34.534623Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot results\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "fig.subplots_adjust(left=0.1, right=0.95, bottom=0.15, top=0.9, wspace=0.25)\n",
    "\n",
    "# First axis shows a projection of the data\n",
    "ax1 = fig.add_subplot(121)\n",
    "im = ax1.scatter(X_test[:, 0], X_test[:, 1], c=y_test, s=4,\n",
    "                 linewidths=0, edgecolors='none',\n",
    "                 cmap=plt.cm.jet)\n",
    "im.set_clim(-0.5, 1)\n",
    "ax1.set_xlim(0.8, 1.5)\n",
    "ax1.set_ylim(-0.5, 0.9)\n",
    "ax1.set_xlabel('$u - g$', fontsize=16)\n",
    "ax1.set_ylabel('$g - i$', fontsize=16)\n",
    "\n",
    "labels = dict(GaussianNB='GNB',\n",
    "              QuadraticDiscriminantAnalysis='QDA',\n",
    "              KNeighborsClassifier='KNN',\n",
    "              DecisionTreeClassifier='DT',\n",
    "              GMMBayes='GMMB',\n",
    "              LogisticRegression='LR')\n",
    "\n",
    "# Second axis shows the ROC curves\n",
    "ax2 = fig.add_subplot(122)\n",
    "for name, y_prob in zip(names, probs):\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "    fpr = np.concatenate([[0], fpr])\n",
    "    tpr = np.concatenate([[0], tpr])\n",
    "    ax2.plot(fpr, tpr, label=labels[name])\n",
    "\n",
    "ax2.legend(loc=4)\n",
    "ax2.set_xlabel('false positive rate', fontsize=16)\n",
    "ax2.set_ylabel('true positive rate', fontsize=16)\n",
    "ax2.set_xlim(0, 0.45)\n",
    "ax2.set_ylim(0.0, 1.01)\n",
    "ax2.xaxis.set_major_locator(plt.MaxNLocator(5))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So, how well can we separate RR Lyrae from the rest of the sample? \n",
    "\n",
    "The answer of course depends where you are on the ROC curve. Since GMMB performs the best, let's look at its ROC curve: if you care about completeness (i.e. TPR), you pick FPR=0.1 and get TPR>0.99. On the other hand, if you care about a clean sample (low FPR), you pick the point with TPR=0.8 and FPR=0.01 (approximately). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
